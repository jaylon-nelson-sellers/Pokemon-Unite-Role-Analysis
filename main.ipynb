{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n",
      "Processing complete. Results saved to 'clustered_main_data.csv' and 'TSNE_clustering_images.png'.\n"
     ]
    }
   ],
   "source": [
    "# Helper Functions #\n",
    "####################\n",
    "# Function to load and resize image\n",
    "\n",
    "\n",
    "def preprocess_data(text,data_name):\n",
    "    def get_image(path, zoom=0.3):\n",
    "        return OffsetImage(plt.imread(path), zoom=zoom)\n",
    "\n",
    "    # Function to lemmatize text using spaCy\n",
    "    def lemmatize_text(text):\n",
    "        doc = nlp(text)\n",
    "        return \" \".join([token.lemma_ for token in doc])\n",
    "    if text:\n",
    "        # Load spaCy model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "     # Load the data\n",
    "    data = pd.read_csv(data_name)\n",
    "    \n",
    "    \n",
    "    # Extract character names and remove from main data\n",
    "    character_names = data.iloc[:, 0].tolist()\n",
    "    main_data = data.iloc[:, 1:]\n",
    "\n",
    "    # Identify numeric and text columns\n",
    "    numeric_columns = main_data.select_dtypes(include=[np.number]).columns\n",
    "    text_columns = main_data.select_dtypes(exclude=[np.number]).columns\n",
    "    print(text_columns)\n",
    "    # Preprocess numeric columns\n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    numeric_data = pd.DataFrame(scaler.fit_transform(imputer.fit_transform(main_data[numeric_columns])), \n",
    "                                columns=numeric_columns)\n",
    "   \n",
    "\n",
    "    # Preprocess text columns using TfidfVectorizer and spaCy lemmatization\n",
    "    if text:\n",
    "        tfidf = TfidfVectorizer()  # You can adjust max_features as needed\n",
    "        text_data = main_data[text_columns].fillna('')\n",
    "        text_data_combined = text_data.apply(lambda x: ' '.join(x), axis=1)\n",
    "        lemmatized_text = text_data_combined.apply(lemmatize_text)\n",
    "        tfidf_matrix = tfidf.fit_transform(lemmatized_text)\n",
    "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "        processed_data = pd.concat([numeric_data,tfidf_df], axis=1)\n",
    "    else:\n",
    "        # Combine processed data \n",
    "        processed_data = pd.concat([numeric_data,], axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "    return [processed_data,character_names]\n",
    "\n",
    "\n",
    "\n",
    "def decompose_data(processed_data,character_names):\n",
    "    \n",
    "    tsne = PCA(n_components=1)\n",
    "    tsne_1d = tsne.fit_transform(processed_data)\n",
    "    tsne_1d = pd.DataFrame(tsne_1d, columns=['x'])\n",
    "    tsne_1d.insert(loc = 0,\n",
    "          column = \"Character\",\n",
    "          value = character_names)\n",
    "    tsne_1d.to_csv(\"data-1d.csv\",index=False)\n",
    "\n",
    "    tsne2 = PCA(n_components=2)\n",
    "    tsne_2d = tsne2.fit_transform(processed_data)\n",
    "    tsne_2d = pd.DataFrame(tsne_2d, columns=['x', 'y'])\n",
    "    tsne_2d.insert(loc = 0,\n",
    "          column = \"Character\",\n",
    "          value = character_names)\n",
    "    tsne_2d.to_csv(\"data-2d.csv\",index=False)\n",
    "    \n",
    "    # Create a graph with images\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    scatter = plt.scatter(tsne_2d['x'], tsne_2d['y'], alpha=0)\n",
    "\n",
    "    for i, character in enumerate(tsne_2d['Character']):\n",
    "        try:\n",
    "            img_path = os.path.join('pokemon_images', f\"{character.lower().replace(' ', '_')}.png\")\n",
    "            ab = AnnotationBbox(get_image(img_path), (tsne_2d['x'][i], tsne_2d['y'][i]), frameon=False)\n",
    "            plt.gca().add_artist(ab)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found for {character}\")\n",
    "\n",
    "    plt.suptitle('Pokemon Unite Character Analysis', fontsize=36)\n",
    "    plt.title('Win Rate, Pick Rate, and Ban Rate converged into 2 Dimensions', fontsize=28)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('KPCA_clustering_images.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Processing complete. Results saved to 'clustered_main_data.csv' and 'TSNE_clustering_images.png'.\")\n",
    "\n",
    "\n",
    "df,chars = preprocess_data(False,\"raw.csv\")\n",
    "decompose_data(df,chars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"Pokemon Data.csv\")\n",
    "# Drop the first column and normalize the remaining data\n",
    "labels = data.iloc[:,0]\n",
    "data_values = data.drop(columns=['Pokemon'])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_values)\n",
    "\n",
    "# Perform PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Perform Kernel PCA with 2 components\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf')\n",
    "kpca_data = kpca.fit_transform(scaled_data)\n",
    "\n",
    "# Convert PCA and KPCA results to dataframes for easy handling\n",
    "pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])\n",
    "kpca_df = pd.DataFrame(kpca_data, columns=['KPCA1', 'KPCA2'])\n",
    "\n",
    "# Display the first few rows of the PCA and KPCA results\n",
    "(labels.head(),pca_df.head(), kpca_df.head())\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Elbow method to determine the optimal number of clusters\n",
    "def elbow_method(data):\n",
    "    sse = []\n",
    "    for k in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(data)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    return sse\n",
    "\n",
    "# Plot the elbow method for PCA data\n",
    "reg_sse = elbow_method(scaled_data)\n",
    "pca_sse = elbow_method(pca_df)\n",
    "kpca_sse = elbow_method(kpca_df)\n",
    "\n",
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "plt.plot(range(1, 11), reg_sse, marker='p', label='Raw')\n",
    "plt.plot(range(1, 11), pca_sse, marker='o', label='PCA')\n",
    "plt.plot(range(1, 11), kpca_sse, marker='s', label='KPCA')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "data = pd.read_csv(\"Pokemon Data.csv\")\n",
    "# Drop the first column and normalize the remaining data\n",
    "labels = data.iloc[:,0]\n",
    "data_values = data.drop(columns=['Pokemon'])\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_values)\n",
    "\n",
    "# Perform PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Perform Kernel PCA with 2 components\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf')\n",
    "kpca_data = kpca.fit_transform(scaled_data)\n",
    "\n",
    "# Convert PCA and KPCA results to dataframes for easy handling\n",
    "pca_df = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])\n",
    "kpca_df = pd.DataFrame(kpca_data, columns=['PC1', 'PC2'])\n",
    "\n",
    "clustering = KMeans(n_clusters=8).fit(pca_df)\n",
    "print()\n",
    "final = pd.concat([labels, pd.DataFrame(pca_df),pd.DataFrame(clustering.labels_) ], axis=1)\n",
    "print(final)\n",
    "# Plot the clustered KPCA data\n",
    "plt.figure(figsize=(12, 6), dpi=500)\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], c=clustering.labels_, cmap='viridis', marker='o')\n",
    "for i, txt in enumerate(labels):\n",
    "    plt.annotate(txt, (pca_df['PC1'][i]+.005, pca_df['PC2'][i]+0.005), fontsize=4, alpha=0.7)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.title('Pokemon Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
